{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.desired_capabilities import  DesiredCapabilities\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파파고 크롤링\n",
    "def kor_to_trans(text_data, trans_lang,start_index,final_index):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"detach\",True)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "\n",
    "    for i in tqdm(range(start_index,final_index)): \n",
    "    \n",
    "      if (i!=0)&(i%99==0):\n",
    "        driver.implicitly_wait(1)\n",
    "        #print('{}th : '.format(i), backtrans)\n",
    "        np.save('p_kor_to_eng_train_{}_{}.npy'.format(start_index,final_index),trans_list)\n",
    "    \n",
    "      try:\n",
    "        driver.get('https://papago.naver.com/?sk=ko&tk='+trans_lang+'&st='+text_data[i])\n",
    "        time.sleep(1)\n",
    "        element=WebDriverWait(driver, 5).until(target_present)\n",
    "        time.sleep(0.1)\n",
    "        backtrans = element.text \n",
    "\n",
    "        if (backtrans=='')|(backtrans==' '):\n",
    "            element=WebDriverWait(driver, 5).until(target_present)\n",
    "            backtrans = element.text \n",
    "            trans_list.append(backtrans)\n",
    "        else:\n",
    "            trans_list.append(backtrans)\n",
    "    \n",
    "      except:\n",
    "        trans_list.append('')\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list=[]\n",
    "kor_to_trans(train['text'], 'en',0,1000)\n",
    "np.save('p_kor_to_eng_train_0_1000.npy',trans_list)\n",
    "trans_list=[]\n",
    "kor_to_trans(train['text'], 'en',1000,2000)\n",
    "np.save('p_kor_to_eng_train_1000_2000.npy',trans_list)\n",
    "trans_list=[]\n",
    "kor_to_trans(train['text'], 'en',2000,3000)\n",
    "np.save('p_kor_to_eng_train_2000_3000.npy',trans_list)\n",
    "trans_list=[]\n",
    "kor_to_trans(train['text'], 'en',3000,4000)\n",
    "np.save('p_kor_to_eng_train_3000_4000.npy',trans_list)\n",
    "trans_list=[]\n",
    "kor_to_trans(train['text'], 'en',4000,5000)\n",
    "np.save('p_kor_to_eng_train_4000_5000.npy',trans_list)\n",
    "trans_list=[]\n",
    "kor_to_trans(train['text'], 'en',5000,6000)\n",
    "np.save('p_kor_to_eng_train_5000_6000.npy',trans_list)\n",
    "trans_list=[]\n",
    "kor_to_trans(train['text'], 'en',6000,7000)\n",
    "np.save('p_kor_to_eng_train_6000_7000.npy',trans_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data0 = np.load('p_kor_to_eng_train_0_1000.npy')\n",
    "eng_data1 = np.load('p_kor_to_eng_train_1000_2000.npy')\n",
    "eng_data2 = np.load('p_kor_to_eng_train_2000_3000.npy')\n",
    "eng_data3 = np.load('p_kor_to_eng_train_3000_4000.npy')\n",
    "eng_data4 = np.load('p_kor_to_eng_train_4000_5000.npy')\n",
    "eng_data5 = np.load('p_kor_to_eng_train_5000_6000.npy')\n",
    "eng_data6 = np.load('p_kor_to_eng_train_6000_7000.npy')\n",
    "\n",
    "eng_data=[*eng_data0,*eng_data1,*eng_data2,*eng_data3,*eng_data4,*eng_data5,*eng_data6]\n",
    "eng_data=pd.DataFrame(eng_data,columns=['eng_sentence'])\n",
    "back_train=pd.concat([train,eng_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_to_kor(transed_list, transed_lang,start_index,final_index):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"detach\",True)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "    \n",
    "    for i in tqdm(range(start_index,final_index)):\n",
    "        if (i!=0)&(i%99==0):\n",
    "            driver.implicitly_wait(1)\n",
    "            np.save('p_eng_to_kor_train_{}_{}.npy'.format(start_index,final_index),trans_list)\n",
    "\n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=en&tk='+transed_lang+'&st='+transed_list[i])\n",
    "            time.sleep(1)\n",
    "            element=WebDriverWait(driver, 5).until(target_present)\n",
    "            time.sleep(0.1)\n",
    "            backtrans = element.text\n",
    "\n",
    "            if (backtrans=='')|(backtrans==' '):\n",
    "                element=WebDriverWait(driver, 5).until(target_present)\n",
    "                backtrans = element.text\n",
    "                trans_list.append(backtrans)\n",
    "            else:\n",
    "                trans_list.append(backtrans)\n",
    " \n",
    "        except:\n",
    "             trans_list.append('')\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_sentence'], 'ko',0,1000)\n",
    "np.save('p_eng_to_ko_train_0_1000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_sentence'], 'ko',1000,2000)\n",
    "np.save('p_eng_to_ko_train_1000_2000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_sentence'], 'ko',2000,3000)\n",
    "np.save('p_eng_to_ko_train_2000_3000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_sentence'], 'ko',3000,4000)\n",
    "np.save('p_eng_to_ko_train_3000_4000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_sentence'], 'ko', 4000,5000)\n",
    "np.save('p_eng_to_ko_train_4000_5000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_sentence'], 'ko', 5000,6000)\n",
    "np.save('p_eng_to_ko_train_5000_6000.npy',trans_list)\n",
    "\n",
    "trans_list=[]\n",
    "trans_to_kor(back_train['eng_sentence'], 'ko', 6000,7000)\n",
    "np.save('p_eng_to_ko_train_6000_7000_end.npy',trans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back0 = np.load('p_eng_to_ko_train_0_1000.npy')\n",
    "back1 = np.load('p_eng_to_ko_train_1000_2000.npy')\n",
    "back2 = np.load('p_eng_to_ko_train_2000_3000.npy')\n",
    "back3 = np.load('p_eng_to_ko_train_3000_4000.npy')\n",
    "back4 = np.load('p_eng_to_ko_train_4000_5000.npy')\n",
    "back5 = np.load('p_eng_to_ko_train_5000_6000.npy')\n",
    "back6 = np.load('p_eng_to_ko_train_6000_7000_end.npy')\n",
    "\n",
    "back_train_fin=[*back0,*back1,*back2,*back3,*back4,*back5,*back6]\n",
    "back_train_fin=pd.DataFrame(back_train_fin,columns=['backtrans_sentence'])\n",
    "back_train_fin=pd.concat([train,back_train_fin],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_train_fin.to_csv('backtrans_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_train_fin = pd.read_csv('./backtrans_train.csv')\n",
    "train= pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan = back_train_fin.dropna(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan[\"ID\"] = not_nan[\"ID\"] +'bt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan['text'] = not_nan['backtrans_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan=not_nan.drop(columns=['backtrans_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bt = pd.concat([train,not_nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bt.to_csv('./train_bt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hanspell 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hanspell.spell_checker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_bt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hanspelling(texts):\n",
    "    \"\"\"\n",
    "    hanspell로 맞춤법 검사를 실시합니다.\n",
    "    \"\"\"\n",
    "    preprocessed_text = []\n",
    "    for text in texts:\n",
    "        text = check(text).checked\n",
    "        if text:\n",
    "            preprocessed_text.append(text)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = hanspelling(train['text'])\n",
    "train['text'] = sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./train_bt_hanspell.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
